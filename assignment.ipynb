{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b2dc3f",
   "metadata": {},
   "source": [
    "# Home assignments (Mykola)\n",
    "\n",
    "### Requirements: \n",
    "- Python==3.11.5\n",
    "- PySpark==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd8705",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "### 1, object store path min/max analysis\n",
    "You have a specific prefix + key structure in your objects store (can be S3, HDFS, ...), that looks like this:\n",
    "`protocol://bucket/base_path/specific_path/keys`  and a key has a structure of `id=some_value/month=yyyy-MM-dd/object{1, 2, 3, ...}`\n",
    "\n",
    "Example:\n",
    "\n",
    "s3://my-bucket/xxx/yyy/zzz/abc/id=123/month=2019-01-01/2019-01-19T10:31:18.818Z.gz\n",
    "\n",
    "s3://my-bucket/xxx/yyy/zzz/abc/id=123/month=2019-02-01/2019-02-19T10:32:18.818Z.gz\n",
    "\n",
    "s3://my-bucket/xxx/yyy/zzz/abc/id=333/month=2019-03-01/2019-06-19T10:33:18.818Z.gz\n",
    "\n",
    "s3://my-bucket/xxx/yyy/zzz/def/id=123/month=2019-10-01/2019-10-19T10:34:18.818Z.gz\n",
    "\n",
    "s3://my-bucket/xxx/yyy/zzz/def/id=333/month=2019-11-01/2019-12-19T10:35:18.818Z.gz\n",
    "\n",
    "You have a function `get_all_keys(bucket, full_path) -> Iterator[str]` for getting all the keys for a full path (base_path + specific_path).\n",
    "\n",
    "Notes:\n",
    "On the input you know your bucket, base_path and all the specific paths you want to generate output for.\n",
    "Also as shown in the example the month subkey has format of a date, but it's always yyyy-MM-01, so effectively it only gives you information about the year and month. Objects (files) within this structure have a timestamp, but this is a timestamp of when they have been created. For illustration, the last line in the example is an object (file) that was generated at '2019-12-19T10:35:18.818Z', but data in it are for the id of '333' and month of 2019-11.\n",
    "\n",
    "**For each specific_path (there can be many):**\n",
    " - A, calculate for each id a minimum and maximum month (there cannot be gaps between moths)\n",
    " - B, write the output to a json file\n",
    " - C, there can be gaps between months (missing months), so report them also in some appropriate structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bad1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "from random import randrange, randint, choice\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import string\n",
    "import shutil\n",
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686aaca8",
   "metadata": {},
   "source": [
    "### Generate Data\n",
    "\n",
    "To avoid using real S3, I decided to use a file system for the simulation.\n",
    "\n",
    "- Each directory path will emulate some S3 prefix: bucket_path/base_path/specific_path.\n",
    "- Inside each final directory there is a file with a list of keys: bucket_path/base_path/specific_path/keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9f99ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_RANGE = (2019, date.today().year)\n",
    "\n",
    "\n",
    "def get_random_range(start: int, end: int) -> List[int]:\n",
    "    return sorted([randint(start, end), randint(start, end)])\n",
    "\n",
    "\n",
    "def get_random_month_range():\n",
    "    start_year, end_year = get_random_range(*YEAR_RANGE)\n",
    "    start_month, end_month = get_random_range(1, 12)\n",
    "    return (start_year, start_month), (end_year, end_month)\n",
    "\n",
    "\n",
    "def generate_path(depth=3, path=Path()):\n",
    "    if depth == 0:\n",
    "        return path\n",
    "    path /= choice(string.ascii_lowercase) * 3\n",
    "    return generate_path(depth - 1, path)\n",
    "\n",
    "\n",
    "def calc_month_range(month_min_obj, month_max_obj):\n",
    "    start_year, min_month = month_min_obj\n",
    "    end_year, max_month = month_max_obj\n",
    "    \n",
    "    month_range = list()\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        \n",
    "        start_month = min_month if year == start_year else 1\n",
    "        end_month = max_month if year == end_year else 12\n",
    "        \n",
    "        for month in range(start_month, end_month + 1):\n",
    "            month_range.append((year, month))\n",
    "        \n",
    "    return month_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09f5e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'119': {'month_gaps': ['2023-07-01', '2021-11-01'],\n",
      "         'month_max': '2023-08-01',\n",
      "         'month_min': '2020-03-01'},\n",
      " '328': {'month_gaps': ['2020-12-01', '2022-08-01'],\n",
      "         'month_max': '2023-09-01',\n",
      "         'month_min': '2019-04-01'},\n",
      " '425': {'month_gaps': [],\n",
      "         'month_max': '2019-10-01',\n",
      "         'month_min': '2019-10-01'},\n",
      " '648': {'month_gaps': ['2022-03-01', '2020-08-01'],\n",
      "         'month_max': '2023-08-01',\n",
      "         'month_min': '2019-01-01'},\n",
      " '696': {'month_gaps': ['2021-10-01'],\n",
      "         'month_max': '2022-04-01',\n",
      "         'month_min': '2020-02-01'}}\n",
      "{'007': {'month_gaps': ['2021-11-01'],\n",
      "         'month_max': '2022-04-01',\n",
      "         'month_min': '2020-03-01'},\n",
      " '011': {'month_gaps': ['2020-10-01'],\n",
      "         'month_max': '2021-12-01',\n",
      "         'month_min': '2019-03-01'},\n",
      " '449': {'month_gaps': ['2021-09-01'],\n",
      "         'month_max': '2023-02-01',\n",
      "         'month_min': '2020-01-01'},\n",
      " '693': {'month_gaps': [],\n",
      "         'month_max': '2022-04-01',\n",
      "         'month_min': '2021-03-01'},\n",
      " '754': {'month_gaps': ['2020-10-01', '2022-06-01'],\n",
      "         'month_max': '2023-03-01',\n",
      "         'month_min': '2019-02-01'}}\n",
      "{'055': {'month_gaps': [],\n",
      "         'month_max': '2023-09-01',\n",
      "         'month_min': '2022-08-01'},\n",
      " '381': {'month_gaps': ['2023-05-01', '2021-09-01'],\n",
      "         'month_max': '2023-08-01',\n",
      "         'month_min': '2020-01-01'},\n",
      " '455': {'month_gaps': [],\n",
      "         'month_max': '2019-09-01',\n",
      "         'month_min': '2019-08-01'},\n",
      " '778': {'month_gaps': [],\n",
      "         'month_max': '2023-06-01',\n",
      "         'month_min': '2023-04-01'},\n",
      " '797': {'month_gaps': ['2022-03-01', '2020-08-01'],\n",
      "         'month_max': '2022-10-01',\n",
      "         'month_min': '2019-01-01'}}\n",
      "{'428': {'month_gaps': ['2020-12-01', '2022-07-01'],\n",
      "         'month_max': '2023-09-01',\n",
      "         'month_min': '2019-05-01'},\n",
      " '456': {'month_gaps': ['2023-08-01'],\n",
      "         'month_max': '2023-12-01',\n",
      "         'month_min': '2021-12-01'},\n",
      " '461': {'month_gaps': ['2022-11-01'],\n",
      "         'month_max': '2023-04-01',\n",
      "         'month_min': '2021-03-01'},\n",
      " '890': {'month_gaps': ['2020-10-01'],\n",
      "         'month_max': '2021-07-01',\n",
      "         'month_min': '2019-03-01'},\n",
      " '999': {'month_gaps': ['2021-12-01'],\n",
      "         'month_max': '2022-12-01',\n",
      "         'month_min': '2020-04-01'}}\n",
      "{'069': {'month_gaps': [],\n",
      "         'month_max': '2021-04-01',\n",
      "         'month_min': '2020-02-01'},\n",
      " '267': {'month_gaps': [],\n",
      "         'month_max': '2023-10-01',\n",
      "         'month_min': '2022-07-01'},\n",
      " '449': {'month_gaps': ['2022-09-01'],\n",
      "         'month_max': '2023-11-01',\n",
      "         'month_min': '2021-01-01'},\n",
      " '706': {'month_gaps': [],\n",
      "         'month_max': '2022-05-01',\n",
      "         'month_min': '2021-04-01'},\n",
      " '755': {'month_gaps': [],\n",
      "         'month_max': '2023-07-01',\n",
      "         'month_min': '2022-04-01'}}\n",
      "{'134': {'month_gaps': [],\n",
      "         'month_max': '2020-08-01',\n",
      "         'month_min': '2020-02-01'},\n",
      " '220': {'month_gaps': [],\n",
      "         'month_max': '2021-09-01',\n",
      "         'month_min': '2021-02-01'},\n",
      " '288': {'month_gaps': ['2022-12-01'],\n",
      "         'month_max': '2023-08-01',\n",
      "         'month_min': '2021-05-01'},\n",
      " '366': {'month_gaps': ['2021-10-01'],\n",
      "         'month_max': '2022-02-01',\n",
      "         'month_min': '2020-02-01'},\n",
      " '824': {'month_gaps': ['2020-11-01'],\n",
      "         'month_max': '2022-04-01',\n",
      "         'month_min': '2019-03-01'}}\n"
     ]
    }
   ],
   "source": [
    "def format_output(result):\n",
    "    for _id in result:\n",
    "        template = \"{}-{:02d}-01\"\n",
    "        result[_id]['month_min'] = template.format(*result[_id]['month_min'])\n",
    "        result[_id]['month_max'] = template.format(*result[_id]['month_max'])\n",
    "        result[_id]['month_gaps'] = [template.format(*month) for month in result[_id]['month_gaps']]\n",
    "\n",
    "\n",
    "def generate_keys(bucket, full_path, id_count=5, max_size=1000, skip_threshold=0.05):\n",
    "    file_path = bucket / full_path / 'keys'\n",
    "    json_output_path = bucket / full_path / 'correct_output.json'\n",
    "    \n",
    "    correct_result = {}\n",
    "    \n",
    "    with file_path.open('a+') as f:\n",
    "        for _ in range(id_count):\n",
    "            _id = f\"{randrange(1, 10**3):03}\"\n",
    "\n",
    "            month_min_obj, month_max_obj = get_random_month_range()\n",
    "            month_list = list(calc_month_range(month_min_obj, month_max_obj))\n",
    "            skip_every = int(len(month_list) / (len(month_list) * skip_threshold))\n",
    "            \n",
    "            correct_result[_id] = {\n",
    "                'month_min': month_list[0],\n",
    "                'month_max':month_list[-1],\n",
    "                'month_gaps': set()\n",
    "            }\n",
    "                \n",
    "            for idx, (year, month) in enumerate(calc_month_range(month_min_obj, month_max_obj)):\n",
    "                if idx % skip_every == 0 and idx != 0 and idx != len(month_list) - 1:\n",
    "                    correct_result[_id]['month_gaps'].add((year, month))\n",
    "                    continue\n",
    "                month_key = \"{}-{:02d}-01\".format(year, month)\n",
    "                file_timestamp = datetime.now().replace(year=year, month=month)\n",
    "                file_name = file_timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                f.write(f\"id={_id}/month={month_key}/{file_name}.gz\\n\")\n",
    "                \n",
    "    format_output(correct_result)\n",
    "    pprint.pprint(correct_result)\n",
    "    with open(json_output_path, 'w') as f:\n",
    "        json.dump(obj=correct_result, fp=f,indent=4)\n",
    "            \n",
    "    \n",
    "def generate_data(bucket_name, count_base_path=5, count_specific_path=10):\n",
    "    input_data = []\n",
    "    \n",
    "    bucket_path = Path(bucket_name)\n",
    "    \n",
    "    if bucket_path.exists() and bucket_path.is_dir():\n",
    "        shutil.rmtree(bucket_path)\n",
    "    \n",
    "    base_path_list = set(generate_path() for _ in range(count_base_path))\n",
    "    specific_path_list = set(generate_path(depth=1) for _ in range(count_specific_path))\n",
    "    \n",
    "    for base_path in base_path_list:\n",
    "        base_path = generate_path()\n",
    "        for specific_path in specific_path_list:\n",
    "            full_path = base_path / specific_path\n",
    "            Path(bucket_path / full_path).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            generate_keys(bucket_path, full_path)\n",
    "            \n",
    "            input_data.append((bucket_path, base_path, specific_path))\n",
    "            \n",
    "    return input_data\n",
    "\n",
    "\n",
    "input_data = generate_data('my-bucket', count_base_path=2, count_specific_path=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72976a35",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e298237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_keys(bucket, full_path):\n",
    "    with (bucket / full_path / 'keys').open() as f:\n",
    "        for line in f:\n",
    "            yield line.strip()\n",
    "\n",
    "\n",
    "def calc_month_range(month_min_obj, month_max_obj):\n",
    "    start_year, min_month = month_min_obj\n",
    "    end_year, max_month = month_max_obj\n",
    "    \n",
    "    month_range = set()\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        \n",
    "        start_month = min_month if year == start_year else 1\n",
    "        end_month = max_month if year == end_year else 12\n",
    "        \n",
    "        for month in range(start_month, end_month + 1):\n",
    "            month_range.add((year, month))\n",
    "        \n",
    "    return month_range\n",
    "\n",
    "\n",
    "def calc_min_max(bucket_path, base_path, specific_path):\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for key in get_all_keys(bucket_path, base_path / specific_path):\n",
    "        # maybe I should use regex and do some validation, I decided to skip\n",
    "        id_data, month_data, _ = key.split('/')\n",
    "        _id = id_data.split('id=')[-1]\n",
    "        \n",
    "        # several options here: \n",
    "        # 1) We can use some libraries to parse dates and increment by month\n",
    "        # 2) Work with integers in python\n",
    "        year, month, _ = month_data.split('month=')[-1].split('-')\n",
    "        month_obj = (int(year), int(month))\n",
    "        \n",
    "        if _id not in result:\n",
    "            result[_id] = {\n",
    "                'month_min': month_obj,\n",
    "                'month_max': month_obj,\n",
    "                'month_gaps': set()\n",
    "            }\n",
    "        else:\n",
    "            result[_id]['month_min'] = min(result[_id]['month_min'], month_obj)\n",
    "            result[_id]['month_max'] = max(result[_id]['month_max'], month_obj)\n",
    "        result[_id]['month_gaps'].add(month_obj)\n",
    "        \n",
    "    \n",
    "        \n",
    "    for _id in result:\n",
    "        month_min = result[_id]['month_min']\n",
    "        month_max = result[_id]['month_max']\n",
    "        month_range = calc_month_range(month_min, month_max)\n",
    "        \n",
    "        result[_id]['month_gaps'] = month_range - result[_id]['month_gaps']\n",
    "        template = \"{}-{:02d}-01\"\n",
    "        \n",
    "        result[_id]['month_min'] = template.format(*month_min)\n",
    "        result[_id]['month_max'] = template.format(*month_max)\n",
    "        result[_id]['month_gaps'] = [template.format(*month) for month in result[_id]['month_gaps']]\n",
    "        \n",
    "    with open(bucket_path / base_path / specific_path / 'output.json', 'w') as f:\n",
    "        json.dump(obj=result, fp=f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255ca98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: my-bucket\\zzz\\jjj\\mmm\\yyy\n",
      "{'119': {'month_gaps': ['2023-07-01', '2021-11-01'],\n",
      "         'month_max': '2023-08-01',\n",
      "         'month_min': '2020-03-01'},\n",
      " '328': {'month_gaps': ['2020-12-01', '2022-08-01'],\n",
      "         'month_max': '2023-09-01',\n",
      "         'month_min': '2019-04-01'},\n",
      " '425': {'month_gaps': [],\n",
      "         'month_max': '2019-10-01',\n",
      "         'month_min': '2019-10-01'},\n",
      " '648': {'month_gaps': ['2022-03-01', '2020-08-01'],\n",
      "         'month_max': '2023-08-01',\n",
      "         'month_min': '2019-01-01'},\n",
      " '696': {'month_gaps': ['2021-10-01'],\n",
      "         'month_max': '2022-04-01',\n",
      "         'month_min': '2020-02-01'}}\n",
      "Location: my-bucket\\zzz\\jjj\\mmm\\fff\n",
      "{'007': {'month_gaps': ['2021-11-01'],\n",
      "         'month_max': '2022-04-01',\n",
      "         'month_min': '2020-03-01'},\n",
      " '011': {'month_gaps': ['2020-10-01'],\n",
      "         'month_max': '2021-12-01',\n",
      "         'month_min': '2019-03-01'},\n",
      " '449': {'month_gaps': ['2021-09-01'],\n",
      "         'month_max': '2023-02-01',\n",
      "         'month_min': '2020-01-01'},\n",
      " '693': {'month_gaps': [],\n",
      "         'month_max': '2022-04-01',\n",
      "         'month_min': '2021-03-01'},\n",
      " '754': {'month_gaps': ['2020-10-01', '2022-06-01'],\n",
      "         'month_max': '2023-03-01',\n",
      "         'month_min': '2019-02-01'}}\n",
      "Location: my-bucket\\zzz\\jjj\\mmm\\qqq\n",
      "{'055': {'month_gaps': [],\n",
      "         'month_max': '2023-09-01',\n",
      "         'month_min': '2022-08-01'},\n",
      " '381': {'month_gaps': ['2023-05-01', '2021-09-01'],\n",
      "         'month_max': '2023-08-01',\n",
      "         'month_min': '2020-01-01'},\n",
      " '455': {'month_gaps': [],\n",
      "         'month_max': '2019-09-01',\n",
      "         'month_min': '2019-08-01'},\n",
      " '778': {'month_gaps': [],\n",
      "         'month_max': '2023-06-01',\n",
      "         'month_min': '2023-04-01'},\n",
      " '797': {'month_gaps': ['2022-03-01', '2020-08-01'],\n",
      "         'month_max': '2022-10-01',\n",
      "         'month_min': '2019-01-01'}}\n",
      "Location: my-bucket\\ddd\\bbb\\qqq\\yyy\n",
      "{'428': {'month_gaps': ['2020-12-01', '2022-07-01'],\n",
      "         'month_max': '2023-09-01',\n",
      "         'month_min': '2019-05-01'},\n",
      " '456': {'month_gaps': ['2023-08-01'],\n",
      "         'month_max': '2023-12-01',\n",
      "         'month_min': '2021-12-01'},\n",
      " '461': {'month_gaps': ['2022-11-01'],\n",
      "         'month_max': '2023-04-01',\n",
      "         'month_min': '2021-03-01'},\n",
      " '890': {'month_gaps': ['2020-10-01'],\n",
      "         'month_max': '2021-07-01',\n",
      "         'month_min': '2019-03-01'},\n",
      " '999': {'month_gaps': ['2021-12-01'],\n",
      "         'month_max': '2022-12-01',\n",
      "         'month_min': '2020-04-01'}}\n",
      "Location: my-bucket\\ddd\\bbb\\qqq\\fff\n",
      "{'069': {'month_gaps': [],\n",
      "         'month_max': '2021-04-01',\n",
      "         'month_min': '2020-02-01'},\n",
      " '267': {'month_gaps': [],\n",
      "         'month_max': '2023-10-01',\n",
      "         'month_min': '2022-07-01'},\n",
      " '449': {'month_gaps': ['2022-09-01'],\n",
      "         'month_max': '2023-11-01',\n",
      "         'month_min': '2021-01-01'},\n",
      " '706': {'month_gaps': [],\n",
      "         'month_max': '2022-05-01',\n",
      "         'month_min': '2021-04-01'},\n",
      " '755': {'month_gaps': [],\n",
      "         'month_max': '2023-07-01',\n",
      "         'month_min': '2022-04-01'}}\n",
      "Location: my-bucket\\ddd\\bbb\\qqq\\qqq\n",
      "{'134': {'month_gaps': [],\n",
      "         'month_max': '2020-08-01',\n",
      "         'month_min': '2020-02-01'},\n",
      " '220': {'month_gaps': [],\n",
      "         'month_max': '2021-09-01',\n",
      "         'month_min': '2021-02-01'},\n",
      " '288': {'month_gaps': ['2022-12-01'],\n",
      "         'month_max': '2023-08-01',\n",
      "         'month_min': '2021-05-01'},\n",
      " '366': {'month_gaps': ['2021-10-01'],\n",
      "         'month_max': '2022-02-01',\n",
      "         'month_min': '2020-02-01'},\n",
      " '824': {'month_gaps': ['2020-11-01'],\n",
      "         'month_max': '2022-04-01',\n",
      "         'month_min': '2019-03-01'}}\n"
     ]
    }
   ],
   "source": [
    "for bucket_path, base_path, specific_path in input_data:\n",
    "    full_path = bucket_path / base_path / specific_path\n",
    "    calc_min_max(bucket_path, base_path, specific_path)\n",
    "    print(f\"Location: {full_path}\")\n",
    "    \n",
    "    with open(full_path / 'output.json', 'r') as f1, open(full_path / 'correct_output.json', 'r') as f2:\n",
    "        a = json.load(f1)\n",
    "        b = json.load(f2)\n",
    "        pprint.pprint(a)\n",
    "        assert a == b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e9340",
   "metadata": {},
   "source": [
    "### 2, parallel upload\n",
    "\n",
    "Your task is to upload data from an object store (can be S3, HDFS, ...) to Elastic.\n",
    "Elastic server has a given number of data nodes e.x. 5 and each one of the nodes will hold some of the indices you want to upload your data into. You can get the info by an API call such as `http://your-elastic-server:port/_cat/shards` this will get you an output of all the indices and details about them in a structure: name_of_the_index, (some unimportant stats), data_node\n",
    "\n",
    "Example:\n",
    "- index-2018-01 … data-node-01\n",
    "- index-2018-02 … data-node-03\n",
    "- index-2018-03 … data-node-02\n",
    "- index-2018-04 … data-node-04\n",
    "- index-2018-05 … data-node-04\n",
    "- index-2018-06 … data-node-05\n",
    "- index-2018-07 … data-node-01\n",
    "\n",
    "As you can see the distribution is quite random, but it's going to be pretty even across the data-nodes.\n",
    "\n",
    "One specific index will always hold data for a year and month combination e.x. index-2018-01 will have all the data for 2018-01. Luckily your teammates already prepared the data for you with this structure in mind, so your data in object store are partitioned by year and month as you need, e.x. `created_year=2017/created_month=1`, `created_year=2018/created_month=12`, etc. and you also have a function `write_to_elastic_index(df, year, month, target_index)` you can leverage.\n",
    "All you have to do is point our df (dataframe) to the right location, specify the partition filters (year and month) and target_index and it will do the dirty work for you (well, in reality it's going to leverage the elasticsearch-spark library).\n",
    "\n",
    "```python\n",
    "def write_to_elastic_index(df, year, month, target_index) -> None:\n",
    "    \"\"\"Writes data filtered from dataframe (df) by the created_year=/created_month partition filter into given Elastic index.\n",
    "\n",
    "    Example:\n",
    "    # write data from object store partition 'created_year=2018/created_month=1' into Elasticsearch index named 'index-2018-01'\n",
    "    write_to_elastic(df, 2018, 1, 'index-2018-01')\n",
    "\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "To effectively utilize the resources, it makes sense to parallelize the task as much as possible, but you cannot process more than one write request per data-node, otherwise it will crush. Write an application that will handle uploading all the data in the most effective way.\n",
    "You can assume a fixed number of data-nodes (or find out the number based on the API response) and data at least from `2017-01 (created_year=2017/created_month=1)` to `2020-01 (created_year=2020/created_month=1)` without any gaps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cf92c",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Since each data node allows only a single write operation, our task parallelization is constrained by the number of nodes on the server. Each thread will only load indices for a specific node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06506d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(threadName)s:%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Fake function to get {name_of_the_index: data_node} mapping\n",
    "def get_shards(date_range, number_of_nodes=5):\n",
    "    return {\"index-{}-{:02d}\".format(*date): \"data-node-{:02d}\".format(randint(1, number_of_nodes)) for date in date_range}\n",
    "\n",
    "# Fake function to write data to elastic\n",
    "def write_to_elastic_index(df, year, month, target_index) -> None:\n",
    "    \"\"\"Writes data filtered from dataframe (df) by the created_year=/created_month partition filter into given Elastic index.\n",
    "\n",
    "    Example:\n",
    "    # write data from object store partition 'created_year=2018/created_month=1' into Elasticsearch index named 'index-2018-01'\n",
    "    write_to_elastic(df, 2018, 1, 'index-2018-01')\n",
    "    \"\"\"\n",
    "    sleep(0.1)\n",
    "    \n",
    "    \n",
    "# Upload all indices to specific node\n",
    "def upload_indices_to_node(node, target_indices):\n",
    "    for target_index in target_indices:\n",
    "        year, month = map(int, target_index.split('-')[1:])\n",
    "        logger.info(f\"({node}, {target_index}) -> Writing...\")\n",
    "        write_to_elastic_index([], year, month, target_index)\n",
    "        logger.info(f\"({node}, {target_index}) -> Completed\")\n",
    "        \n",
    "\n",
    "\n",
    "# We can reuse date range calculation from the previous task\n",
    "date_range = calc_month_range((2017, 1), (2020, 1))\n",
    "\n",
    "# Number of nodes\n",
    "number_of_nodes = 5\n",
    "\n",
    "# Get index: node mapping\n",
    "shards = get_shards(date_range, number_of_nodes)\n",
    "\n",
    "# Aggregate indices by nodes\n",
    "node_to_index = defaultdict(list)\n",
    "for target_index, node in shards.items():\n",
    "    node_to_index[node].append(target_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981f061",
   "metadata": {},
   "source": [
    "### Parallel threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203ccae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-04) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2019-07) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2019-10) -> Writing...\n",
      "ThreadPoolExecutor-0_3:(data-node-03, index-2019-09) -> Writing...\n",
      "ThreadPoolExecutor-0_4:(data-node-01, index-2017-03) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-04) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-07) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2019-07) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2017-10) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2019-10) -> Completed\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2017-01) -> Writing...\n",
      "ThreadPoolExecutor-0_3:(data-node-03, index-2019-09) -> Completed\n",
      "ThreadPoolExecutor-0_3:(data-node-03, index-2018-07) -> Writing...\n",
      "ThreadPoolExecutor-0_4:(data-node-01, index-2017-03) -> Completed\n",
      "ThreadPoolExecutor-0_4:(data-node-01, index-2018-04) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-07) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-04) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2017-10) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2018-11) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2017-01) -> Completed\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-02) -> Writing...\n",
      "ThreadPoolExecutor-0_3:(data-node-03, index-2018-07) -> Completed\n",
      "ThreadPoolExecutor-0_3:(data-node-03, index-2017-02) -> Writing...\n",
      "ThreadPoolExecutor-0_4:(data-node-01, index-2018-04) -> Completed\n",
      "ThreadPoolExecutor-0_4:(data-node-01, index-2018-03) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-04) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-12) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2018-11) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2019-06) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-02) -> Completed\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-05) -> Writing...\n",
      "ThreadPoolExecutor-0_3:(data-node-03, index-2017-02) -> Completed\n",
      "ThreadPoolExecutor-0_4:(data-node-01, index-2018-03) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-12) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-09) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2019-06) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2017-12) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-05) -> Completed\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-08) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-09) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-06) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2017-12) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2018-10) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-08) -> Completed\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2019-03) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-06) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-05) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2018-10) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2019-02) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2019-03) -> Completed\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-01) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-05) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-11) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2019-02) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2020-01) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-01) -> Completed\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2019-08) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-11) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2018-06) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2020-01) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2017-05) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2019-08) -> Completed\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-12) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2018-06) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-08) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2017-05) -> Completed\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2017-11) -> Writing...\n",
      "ThreadPoolExecutor-0_2:(data-node-04, index-2018-12) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2017-08) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2018-09) -> Writing...\n",
      "ThreadPoolExecutor-0_1:(data-node-05, index-2017-11) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2018-09) -> Completed\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-01) -> Writing...\n",
      "ThreadPoolExecutor-0_0:(data-node-02, index-2019-01) -> Completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=number_of_nodes) as executor:\n",
    "    futures = [executor.submit(upload_indices_to_node, node, target_indices) for node, target_indices in node_to_index.items()]\n",
    "    \n",
    "    for future in futures:\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac4e08",
   "metadata": {},
   "source": [
    "### Single thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e511dc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MainThread:(data-node-02, index-2019-04) -> Writing...\n",
      "MainThread:(data-node-02, index-2019-04) -> Completed\n",
      "MainThread:(data-node-02, index-2017-07) -> Writing...\n",
      "MainThread:(data-node-02, index-2017-07) -> Completed\n",
      "MainThread:(data-node-02, index-2017-04) -> Writing...\n",
      "MainThread:(data-node-02, index-2017-04) -> Completed\n",
      "MainThread:(data-node-02, index-2019-12) -> Writing...\n",
      "MainThread:(data-node-02, index-2019-12) -> Completed\n",
      "MainThread:(data-node-02, index-2017-09) -> Writing...\n",
      "MainThread:(data-node-02, index-2017-09) -> Completed\n",
      "MainThread:(data-node-02, index-2017-06) -> Writing...\n",
      "MainThread:(data-node-02, index-2017-06) -> Completed\n",
      "MainThread:(data-node-02, index-2019-05) -> Writing...\n",
      "MainThread:(data-node-02, index-2019-05) -> Completed\n",
      "MainThread:(data-node-02, index-2019-11) -> Writing...\n",
      "MainThread:(data-node-02, index-2019-11) -> Completed\n",
      "MainThread:(data-node-02, index-2018-06) -> Writing...\n",
      "MainThread:(data-node-02, index-2018-06) -> Completed\n",
      "MainThread:(data-node-02, index-2017-08) -> Writing...\n",
      "MainThread:(data-node-02, index-2017-08) -> Completed\n",
      "MainThread:(data-node-02, index-2018-09) -> Writing...\n",
      "MainThread:(data-node-02, index-2018-09) -> Completed\n",
      "MainThread:(data-node-02, index-2019-01) -> Writing...\n",
      "MainThread:(data-node-02, index-2019-01) -> Completed\n",
      "MainThread:(data-node-05, index-2019-07) -> Writing...\n",
      "MainThread:(data-node-05, index-2019-07) -> Completed\n",
      "MainThread:(data-node-05, index-2017-10) -> Writing...\n",
      "MainThread:(data-node-05, index-2017-10) -> Completed\n",
      "MainThread:(data-node-05, index-2018-11) -> Writing...\n",
      "MainThread:(data-node-05, index-2018-11) -> Completed\n",
      "MainThread:(data-node-05, index-2019-06) -> Writing...\n",
      "MainThread:(data-node-05, index-2019-06) -> Completed\n",
      "MainThread:(data-node-05, index-2017-12) -> Writing...\n",
      "MainThread:(data-node-05, index-2017-12) -> Completed\n",
      "MainThread:(data-node-05, index-2018-10) -> Writing...\n",
      "MainThread:(data-node-05, index-2018-10) -> Completed\n",
      "MainThread:(data-node-05, index-2019-02) -> Writing...\n",
      "MainThread:(data-node-05, index-2019-02) -> Completed\n",
      "MainThread:(data-node-05, index-2020-01) -> Writing...\n",
      "MainThread:(data-node-05, index-2020-01) -> Completed\n",
      "MainThread:(data-node-05, index-2017-05) -> Writing...\n",
      "MainThread:(data-node-05, index-2017-05) -> Completed\n",
      "MainThread:(data-node-05, index-2017-11) -> Writing...\n",
      "MainThread:(data-node-05, index-2017-11) -> Completed\n",
      "MainThread:(data-node-04, index-2019-10) -> Writing...\n",
      "MainThread:(data-node-04, index-2019-10) -> Completed\n",
      "MainThread:(data-node-04, index-2017-01) -> Writing...\n",
      "MainThread:(data-node-04, index-2017-01) -> Completed\n",
      "MainThread:(data-node-04, index-2018-02) -> Writing...\n",
      "MainThread:(data-node-04, index-2018-02) -> Completed\n",
      "MainThread:(data-node-04, index-2018-05) -> Writing...\n",
      "MainThread:(data-node-04, index-2018-05) -> Completed\n",
      "MainThread:(data-node-04, index-2018-08) -> Writing...\n",
      "MainThread:(data-node-04, index-2018-08) -> Completed\n",
      "MainThread:(data-node-04, index-2019-03) -> Writing...\n",
      "MainThread:(data-node-04, index-2019-03) -> Completed\n",
      "MainThread:(data-node-04, index-2018-01) -> Writing...\n",
      "MainThread:(data-node-04, index-2018-01) -> Completed\n",
      "MainThread:(data-node-04, index-2019-08) -> Writing...\n",
      "MainThread:(data-node-04, index-2019-08) -> Completed\n",
      "MainThread:(data-node-04, index-2018-12) -> Writing...\n",
      "MainThread:(data-node-04, index-2018-12) -> Completed\n",
      "MainThread:(data-node-03, index-2019-09) -> Writing...\n",
      "MainThread:(data-node-03, index-2019-09) -> Completed\n",
      "MainThread:(data-node-03, index-2018-07) -> Writing...\n",
      "MainThread:(data-node-03, index-2018-07) -> Completed\n",
      "MainThread:(data-node-03, index-2017-02) -> Writing...\n",
      "MainThread:(data-node-03, index-2017-02) -> Completed\n",
      "MainThread:(data-node-01, index-2017-03) -> Writing...\n",
      "MainThread:(data-node-01, index-2017-03) -> Completed\n",
      "MainThread:(data-node-01, index-2018-04) -> Writing...\n",
      "MainThread:(data-node-01, index-2018-04) -> Completed\n",
      "MainThread:(data-node-01, index-2018-03) -> Writing...\n",
      "MainThread:(data-node-01, index-2018-03) -> Completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for node, target_indices in node_to_index.items():\n",
    "    upload_indices_to_node(node, target_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8461433f",
   "metadata": {},
   "source": [
    "## Spark (solve with Spark 2.4+)\n",
    "\n",
    "### 3, caching\n",
    "\n",
    "Assume we have a parquet file with these three columns: col1, col2, col3 (all have numerical values). Next we create a Spark DataFrame as follows:\n",
    "```python\n",
    "df = spark.read.parquet(path_to_the_data)\n",
    "```\n",
    "\n",
    "In the next step we filter the data and use caching on the filtered DataFrame:\n",
    "```python\n",
    "df.select('col1', 'col2').filter(col('col2') > 100).cache()\n",
    "df.count()\n",
    "```\n",
    "\n",
    "Now we run these three queries:\n",
    "```python\n",
    "1) df.select('col1', 'col2').filter(col('col2') > 101).collect()\n",
    "2) df.select('col1', 'col2').withColumn('col4', lit('test')).filter(col('col2') > 100).collect()\n",
    "2) df.select('col1').filter(col('col2') > 100).collect()\n",
    "```\n",
    "**Which of these three queries will take the data from cache? Please explain your answer.**\n",
    "\n",
    "### Answer\n",
    "We can see if a DataFrame was cached in our physical plan using `explain` operator (where `InMemoryRelation` entities reflect cached datasets with their storage level). \n",
    "\n",
    "Of the three options, only the third returns InMemoryRelation entity. So the correct answer is:\n",
    "```python\n",
    "3) df.select('col1').filter(col('col2') > 100).collect()\n",
    "```\n",
    "\n",
    "In other cases, PySpark will scan original parquet.\n",
    "\n",
    "But we still can manage the caching ourselves. We can enforce the use of caching with a command like the following:\n",
    "```python\n",
    "df2 = df.select('col1', 'col2').filter(col('col2') > 100).cache()\n",
    "df2.count()\n",
    "```\n",
    "However, it is important to carefully analyze our actions; failing to do so could lead to incorrect results.\n",
    "\n",
    "In the first case, we can use the cache because we are only accessing data within the cached dataframe where the values in 'col2' are greater than 100\n",
    "```python\n",
    "df2.select('col1', 'col2').filter(col('col2') > 101).collect()\n",
    "```\n",
    "In the second case, this will also not affect the result. So we can safely use the command:\n",
    "```python\n",
    "df2.select('col1', 'col2').withColumn('col4', lit('test')).filter(col('col2') > 100).collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3a2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, col, lit\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_dummy_data(file_name, size=1E6):\n",
    "    spark = SparkSession.builder.appName(\"Dummy Data Generator\").getOrCreate()\n",
    "    df = spark.range(size)\\\n",
    "        .withColumn(\"col1\", rand() * 1000)\\\n",
    "        .withColumn(\"col2\", rand() * 1000)\\\n",
    "        .withColumn(\"col3\", rand() * 1000)\n",
    "    df.coalesce(10).write.parquet('dummy_data', mode='overwrite')\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca4d37",
   "metadata": {},
   "source": [
    "### Read parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a275074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Read parquet\").getOrCreate()\n",
    "\n",
    "dummy_data_path = 'dummy_data'\n",
    "\n",
    "if not os.path.exists(dummy_data_path):\n",
    "    generate_dummy_data(dummy_data_path)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Read parquet\").getOrCreate()\n",
    "df = spark.read.parquet(dummy_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7874af3",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5416c6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('col1', 'col2').filter(col('col2') > 100).cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c5e81",
   "metadata": {},
   "source": [
    "### 1) `df.select('col1', 'col2').filter(col('col2') > 101).collect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d623d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(col2#2) AND (col2#2 > 101.0))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [col1#1,col2#2] Batched: true, DataFilters: [isnotnull(col2#2), (col2#2 > 101.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nekowaii/Documents/GitHub/Accolade-assignment/dummy_data], PartitionFilters: [], PushedFilters: [IsNotNull(col2), GreaterThan(col2,101.0)], ReadSchema: struct<col1:double,col2:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('col1', 'col2').filter(col('col2') > 101).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00dd566",
   "metadata": {},
   "source": [
    "### 2) `df.select('col1', 'col2').withColumn('col4', lit('test')).filter(col('col2') > 100).collect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013b2c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [col1#1, col2#2, test AS col4#34]\n",
      "+- *(1) Filter (isnotnull(col2#2) AND (col2#2 > 100.0))\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [col1#1,col2#2] Batched: true, DataFilters: [isnotnull(col2#2), (col2#2 > 100.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nekowaii/Documents/GitHub/Accolade-assignment/dummy_data], PartitionFilters: [], PushedFilters: [IsNotNull(col2), GreaterThan(col2,100.0)], ReadSchema: struct<col1:double,col2:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('col1', 'col2').withColumn('col4', lit('test')).filter(col('col2') > 100).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f1854",
   "metadata": {},
   "source": [
    "### * 3) `df.select('col1').filter(col('col2') > 100).collect()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e42b8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- InMemoryTableScan [col1#1]\n",
      "      +- InMemoryRelation [col1#1, col2#2], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(1) Filter (isnotnull(col2#2) AND (col2#2 > 100.0))\n",
      "               +- *(1) ColumnarToRow\n",
      "                  +- FileScan parquet [col1#1,col2#2] Batched: true, DataFilters: [isnotnull(col2#2), (col2#2 > 100.0)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/nekowaii/Documents/GitHub/Accolade-assignment/dummy_data], PartitionFilters: [], PushedFilters: [IsNotNull(col2), GreaterThan(col2,100.0)], ReadSchema: struct<col1:double,col2:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('col1').filter(col('col2') > 100).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5420fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b76c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
